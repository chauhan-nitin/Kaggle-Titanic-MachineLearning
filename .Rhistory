###############################################################################
#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ Ingesting & Preprocessing Datasets ~ ~ ~ ~ ~ ~ ~ ~ ~ ~
###############################################################################
rm(list=ls(all=TRUE))
gc()
#Current working directory
getwd()
# Step 1: Set working directory to where your data files are stored
setwd("C:\\Users\\Nitin.Chauhan\\Documents\\GitHub\\Kaggle-Titanic-MachineLearning")
library(ggplot2)
library(ggrepel)
# Step 2: Import the data and read the file
traindata <- read.csv(file="train.csv", header=TRUE,na.strings=c(""))
testdata <- read.csv(file="test.csv", header = TRUE, na.strings=c(""))
# Save the Survived labels from train data in variable survived
survived <- traindata$Survived
# Step 3: Determine the class and structure of train dataset
class(traindata)
sapply(traindata, function(x) sum(is.na(x)))
# Preprocessing the Train Dataset
traindata$Pclass <- as.factor(traindata$Pclass)
traindata$Age <- as.integer(traindata$Age)
traindata$Name <- as.character(traindata$Name)
str(traindata)
# Step 4: Determine the class and structure of test dataset
class(testdata)
sapply(testdata, function(x) sum(is.na(x)))
# Preprocessing the Test Dataset
testdata$Pclass <- as.factor(testdata$Pclass)
testdata$Age <- as.integer(testdata$Age)
testdata$Name <- as.character(testdata$Name)
str(testdata)
# Step 5: Feature Engineering in Train Data
# Create a new feature Title
traindata$Title <- gsub('(.*, )|(\\..*)', '', traindata$Name)
traindata$Title <- as.factor(traindata$Title)
str(traindata$Title)
# Create a new feature family_size
traindata$family_size <- as.integer(traindata$SibSp + traindata$Parch + 1)
"Drop Cabin since more than 70% values are missing
Drop some of the predictors that are redundant"
traindata <- subset(traindata,select = -c(Name,SibSp,Parch,Ticket,Cabin))
# Identify if any Age is identified as 0
traindata[which(traindata$Age==0),] # Where Age is less than a year either new born or wrongly interpreted
traindata$Age[which(traindata$Age==0)] <- 1 # Since Title are either Master or Miss
levels(traindata$Title)
sort(summary(traindata$Title)[1:17],decreasing = T)
# Create a new variable var holding few titles
var <- c("Mr", "Mrs", "Master", "Miss", "Dr")
# Since there are some titles that are 1 in count causing the system to see some unseen title in test data
length(traindata$Title[!traindata$Title %in% var]) # 20
# Rename the 20 other titles as VIP
traindata$Title <- as.character(traindata$Title)
traindata$Title[!traindata$Title %in% var] <- 'VIP'
traindata$Title <- as.factor(traindata$Title)
# Step 6: Feature Engineering in Test Data
# Create a new feature Title
testdata$Title <- gsub('(.*, )|(\\..*)', '', testdata$Name)
testdata$Title <- as.factor(testdata$Title)
str(testdata$Title)
# Create a new feature family_size
testdata$family_size <- as.integer(testdata$SibSp + testdata$Parch + 1)
"Drop Cabin since more than 70% values are missing
Drop some of the predictors that are redundant"
testdata <- subset(testdata,select = -c(Name,SibSp,Parch,Ticket,Cabin))
# Identify if any Age is identified as 0
testdata[which(testdata$Age==0),] # Where Age is less than a year either new born or wrongly interpreted
testdata$Age[which(testdata$Age==0)] <- 1 # Since Title are either Master or Miss
levels(testdata$Title)
sort(summary(testdata$Title)[1:9],decreasing = T)
# Since there are some titles that are 1 in count causing the system to see some unseen title in test data
length(testdata$Title[!testdata$Title %in% var]) # 6
# Rename the 6 other titles as VIP
testdata$Title <- as.character(testdata$Title)
testdata$Title[!testdata$Title %in% var] <- 'VIP'
testdata$Title <- as.factor(testdata$Title)
"Use the concept of Random Forest to calculate missing values for quantitative
and qualitative predictors. It's a non-parametric approach without any assumptions"
#install.packages("missForest")
library(missForest)
# Step 7: Impute Missing Values in Train Data
sapply(traindata,function(x) sum(is.na(x)))
traindata.imp <- missForest(traindata)
sum(is.na(traindata.imp$ximp))
"Shows the error rate during computation of missing values
Quite an effective way to check quality of computational process"
traindata.imp$OOBerror
traindata <- traindata.imp$ximp
# Step 8: Impute Missing Values in Test Data
sapply(testdata,function(x) sum(is.na(x)))
testdata.imp <- missForest(testdata)
sum(is.na(testdata.imp$ximp))
"Shows the error rate during computation of missing values
Quite an effective way to check quality of computational process"
testdata.imp$OOBerror
testdata <- testdata.imp$ximp
# Step 9: Identify number of observation for each class
count_survived <- qplot(factor(traindata$Survived), fill = factor(traindata$Survived))
count_survived + geom_label_repel(stat='count', aes(label=..count..), nudge_x = 0.0, nudge_y = -50)
count_survived + geom_label_repel(stat='count', aes(label=..count..), nudge_x = 0.0, nudge_y = -50) + labs(title="Count of each Class",
x ="Class", y = "Count")
count_survived + geom_label_repel(stat='count', aes(label=..count..), nudge_x = 0.0, nudge_y = -50) + labs(title="No. of Observation for each class",
x ="Class", y = "Count")
#Plotting relation between Sex & Survived on traindata
ggplot(traindata, aes(x= Sex, fill=factor(Survived))) +
geom_bar(width=0.5) + geom_label_repel(stat='count', aes(label=..count..), nudge_x = 0, nudge_y = 0.2) +
xlab("Sex") +
ylab("Total Count") +
labs(fill="Survived")
###############################################################################
#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ Visualizing the Datasets ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~
###############################################################################
#Plotting relation between Age & Survived on traindata
ggplot(traindata, aes(x= Age, fill=factor(Survived))) +
geom_bar(width=0.5) + labs(title="No. of observation for each class", x ="Age", y = "Total Count", fill="Survived")
